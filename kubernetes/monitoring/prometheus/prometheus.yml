---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |-
    global:
      scrape_interval: 15s
      scrape_timeout: 10s
      evaluation_interval: 15s
    scrape_configs:
    - job_name: 'pushgateway'
      honor_labels: true
      static_configs:
      - targets: ['prometheus-push-gateway.monitoring:8080']
    # - job_name: 'blackbox'
    #   metrics_path: /probe
    #   params:
    #     module: [icmp_ipv4]
    #   static_configs:
    #     - targets: [ '192.168.3.179' ]
    #       labels:
    #           nodeName: 'cam-achtertuin'
    #           jobscope: 'node'
    #     - targets: [ '192.168.3.168' ]
    #       labels:
    #           nodeName: 'cam-voortuin'
    #           jobscope: 'node'
    #     - targets: [ '192.168.6.64' ]
    #       labels:
    #           nodeName: 'oven'
    #           jobscope: 'node'
    #     - targets: [ '192.168.1.37' ]
    #       labels:
    #           nodeName: 'deurbel'
    #           jobscope: 'node'
    #   relabel_configs:
    #     - source_labels: [__address__]
    #       target_label: __param_target
    #     - source_labels: [__param_target]
    #       target_label: instance
    #     - target_label: __address__
    #       replacement: prometheus-blackbox-exporter:80  # This is your blackbox exporter.
    - job_name: 'targetsjson'
      file_sd_configs:
      - files:
        - '/etc/prometheus/targets.json'
    rule_files:
      - /etc/prometheus/rules/*.yml 
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager-service:9093"

    # - job_name: qbittorent
    #   scrape_interval: 5s
    #   static_configs:
    #   - targets: ['exporter-qbittorent.monitoring:80']
    #     labels:
    #       jobscope: 'application'
    # - job_name: openvpn
    #   scrape_interval: 5s
    #   static_configs:
    #   - targets:
    #     - exporter-openvpn:80
    #     labels:
    #       jobscope: 'application'


---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-targets
  namespace: monitoring
data:
  targets.json: |-
    [
      {
        "targets": [
          "localhost:9090"
        ],
        "labels": {
          "targetname": "prometheus-local",
          "jobscope": "application"
        }
      },
      {
        "targets": [
          "192.168.5.1:9100"
        ],
        "labels": {
          "targetname": "linux-wayne",
          "nodeName": "linux-wayne",
          "jobscope": "node",
          "nodetype": "cluster-node"
        }
      },
      {
        "targets": [
          "192.168.5.2:9100"
        ],
        "labels": {
          "targetname": "stephanie",
          "nodeName": "stephanie",
          "jobscope": "node",
          "nodetype": "cluster-node"
        }
      },
      {
        "targets": [
          "192.168.5.3:9100"
        ],
        "labels": {
          "targetname": "jay-c",
          "nodeName": "jay-c",
          "jobscope": "node",
          "nodetype": "cluster-node"
        }
      },
      {
        "targets": [
          "traefik-ingress-service.traefik:8082"
        ],
        "labels": {
          "targetname": "traefik",
          "jobscope": "application"
        }
      },
      {
        "targets": [
          "online-traefik-ingress-lb.online:8082"
        ],
        "labels": {
          "targetname": "traefik-online",
          "jobscope": "application"
        }
      },
      {
        "targets": [
          "kube-state-metrics:8080"
        ],
        "labels": {
          "targetname": "kube-state-metrics",
          "jobscope": "application"
        }
      }
    ]



---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules-general
  namespace: monitoring
data:
  generalrules.yml: |-
    groups:
    # - name: deadmanswitch
    #   rules:
    #   - alert: DeadMansSwitch
    #     annotations:
    #       description: This is a DeadMansSwitch meant to ensure that the entire Alerting
    #         pipeline is functional.
    #       summary: Alerting DeadMansSwitch
    #     expr: vector(1)
    #     labels:
    #       severity: none
    - name: prometheus
      rules:
      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
          description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: PrometheusAlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
          description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: PrometheusNotConnectedToAlertmanager
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
          description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
    - name: node
      rules: 
      - alert: Node down
        expr: up{nodetype="cluster-node"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          title: Node {{ $labels.instance }} is down
          description: Failed to scrape {{ $labels.targetname }} on {{ $labels.instance }} for more than 1 minute. Node seems down.
      - alert: HostOutOfMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host out of memory (instance {{ $labels.instance }})"
          description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      - alert: HostMemoryUnderMemoryPressure
        expr: rate(node_vmstat_pgmajfault[1m]) > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host memory under memory pressure (instance {{ $labels.instance }})"
          description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: HostUnusualNetworkThroughputIn
        expr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host unusual network throughput in (instance {{ $labels.instance }})"
          description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: HostUnusualNetworkThroughputOut
        expr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host unusual network throughput out (instance {{ $labels.instance }})"
          description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail_bytes{mountpoint="/rootfs"}  * 100) / node_filesystem_size_bytes{mountpoint="/rootfs"} < 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host out of disk space (instance {{ $labels.instance }})"
          description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostDiskWillFillIn4Hours
        expr: predict_linear(node_filesystem_free_bytes{fstype!~"tmpfs"}[1h], 4 * 3600) < 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host disk will fill in 4 hours (instance {{ $labels.instance }})"
          description: "Disk will fill in 4 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: HostHighCpuLoad
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host high CPU load (instance {{ $labels.instance }})"
          description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostSystemdServiceCrashed
        expr: node_systemd_unit_state{state="failed"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host SystemD service crashed (instance {{ $labels.instance }})"
          description: "SystemD service crashed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"    

      - alert: HostPhysicalComponentTooHot
        expr: node_hwmon_temp_celsius > 75
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host physical component too hot (instance {{ $labels.instance }})"
          description: "Physical hardware component too hot\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostNodeOvertemperatureAlarm
        expr: node_hwmon_temp_alarm == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Host node overtemperature alarm (instance {{ $labels.instance }})"
          description: "Physical node temperature alarm triggered\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - name: traefik-internal
      rules:
      - alert: TraefikBackendDown
        expr: count(traefik_backend_server_up) by (backend) == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik backend down (instance {{ $labels.instance }})"
          description: "All Traefik backends are down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: TraefikHighHttp4xxErrorRateBackend
        expr: sum(rate(traefik_backend_requests_total{code=~"4.*"}[3m])) by (backend) / sum(rate(traefik_backend_requests_total[3m])) by (backend) * 100 > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik high HTTP 4xx error rate backend (instance {{ $labels.instance }})"
          description: "Traefik backend 4xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: TraefikHighHttp5xxErrorRateBackend
        expr: sum(rate(traefik_backend_requests_total{code=~"5.*"}[3m])) by (backend) / sum(rate(traefik_backend_requests_total[3m])) by (backend) * 100 > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik high HTTP 5xx error rate backend (instance {{ $labels.instance }})"
          description: "Traefik backend 5xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - name: kube-state-metrics
      rules:
      - alert: KubernetesNodeReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
          description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesOutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesStatefulsetDown
        expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})"
          description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesPodNotHealthy
        expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[1h:]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
          description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
          description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesReplicassetMismatch
        expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
          description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesDeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})"
          description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesStatefulsetReplicasMismatch
        expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})"
          description: "A StatefulSet has not matched the expected number of replicas for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesDaemonsetMisscheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})"
          description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    # - name: camera
    #   rules:
    #   - alert: Camera is triggered
    #     expr: camera_trigger == 1
    #     for: 3s
    #     labels:
    #       severity: info
    #     annotations:
    #       title: Camera Triggered
    #       description: The Camera at {{ $labels.instance }} is triggered
      
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: prometheus
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: prometheus
    spec:
      affinity:
        # nodeAffinity:
        #   requiredDuringSchedulingIgnoredDuringExecution :
        #     nodeSelectorTerms:
        #     - matchExpressions:
        #       - key: externalvpn
        #         operator: In
        #         values: [ "true" ]
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 2
            preference:
              matchExpressions:
              - key: la1r.workload/essential
                operator: In
                values:
                - "true"
      containers:
      - name: prometheus
        resources:
        ports:
        - containerPort: 9090
        image: prom/prometheus
        args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.retention.time=1000d
        volumeMounts:
        - name: prometheus-volume
          mountPath: /prometheus
        - name: prometheus-config
          mountPath: /etc/prometheus/prometheus.yml
          subPath: prometheus.yml
        - name: prometheus-targets
          mountPath: /etc/prometheus/targets.json
          subPath: targets.json
        - name: rules-general
          mountPath: /etc/prometheus/rules/
      volumes:
      - name: prometheus-volume
        persistentVolumeClaim:
          claimName: prometheus-claim
      - name: prometheus-config
        configMap:
          name: prometheus-config
          items:
          - key: prometheus.yml
            path: prometheus.yml
          defaultMode: 0744
      - name: prometheus-targets
        configMap:
          name: prometheus-targets
          items:
          - key: targets.json
            path: targets.json
          defaultMode: 0744
      - name: rules-general
        configMap:
          name: prometheus-rules-general
---
apiVersion: v1
kind: Service
metadata:
  labels:
    task: monitoring
  name: prometheus
  namespace: monitoring
spec:
  ports:
  - port: 80
    targetPort: 9090
  selector:
    k8s-app: prometheus
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: prometheus-bas
  namespace: monitoring
spec:
  issuerRef:
    name: la1r
    kind: ClusterIssuer
  secretName: prometheus-bas-tls
  commonName: prometheus.bas
  dnsNames:
  - prometheus.bas
  - www.prometheus.bas

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-https
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: traefik
    traefik.ingress.kubernetes.io/router.tls: "true"
    traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
spec:
  tls:
  - secretName: prometheus-bas-tls
  rules:
  - host: prometheus.bas
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus
            port:
              number: 80

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-http
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: traefik
    traefik.ingress.kubernetes.io/router.tls: "false"
    traefik.ingress.kubernetes.io/router.entrypoints: "web"
    traefik.ingress.kubernetes.io/router.middlewares: "traefik-http-redirect-to-https@kubernetescrd"
spec:
  rules:
  - host: prometheus.bas
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus
            port:
              number: 80

