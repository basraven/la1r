#################################################################################################################
# Define the settings for the rook-ceph cluster with common settings for a production cluster on top of bare metal.

# This example expects three nodes, each with two available disks. Please modify it according to your environment.
# See the documentation for more details on storage settings available.

# For example, to create the cluster:
#   kubectl create -f crds.yaml -f common.yaml -f operator.yaml
#   kubectl create -f cluster-on-local-pvc.yaml
#################################################################################################################
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: rook-config-override
  namespace: rook-ceph # namespace:cluster
data:
  config: |
    [global]
    osd_pool_default_size = 1
    mon_warn_on_pool_no_redundancy = true
    bdev_flock_retry = 20
    bluefs_buffered_io = false
---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  dataDirHostPath: /var/lib/rook
  # placement:
  #   mon:
  #     tolerations:
  #     - key: node-role.kubernetes.io/master
  #       effect: NoSchedule
  mon:
    count: 1
    # placement:
    #   tolerations:
    #   - key: node-role.kubernetes.io/master
    #     effect: NoSchedule
    allowMultiplePerNode: true

    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi
  cephVersion:
    image: quay.io/ceph/ceph:v16.2.6
    allowUnsupported: false
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mgr:
    count: 1
    allowMultiplePerNode: true
    # modules:
    #   - name: pg_autoscaler
    #     enabled: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: false
  storage:
    storageClassDeviceSets:
      - name: set1
        count: 1
        portable: false
        tuneDeviceClass: true
        tuneFastDeviceClass: false
        encrypted: false
        placement:
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
                      - rook-ceph-osd-prepare
        # preparePlacement:
          # podAntiAffinity:
          #   preferredDuringSchedulingIgnoredDuringExecution:
          #     - weight: 100
          #       podAffinityTerm:
          #         labelSelector:
          #           matchExpressions:
          #             - key: app
          #               operator: In
          #               values:
          #                 - rook-ceph-osd
          #             - key: app
          #               operator: In
          #               values:
          #                 - rook-ceph-osd-prepare
          #         topologyKey: kubernetes.io/hostname
        resources:
        # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
        #   limits:
        #     cpu: "500m"
        #     memory: "4Gi"
        #   requests:
        #     cpu: "500m"
        #     memory: "4Gi"
        volumeClaimTemplates:
          - metadata:
              name: data
              # if you are looking at giving your OSD a different CRUSH device class than the one detected by Ceph
              # annotations:
              #   crushDeviceClass: hybrid
            spec:
              resources:
                requests:
                  storage: 10Gi
              # IMPORTANT: Change the storage class depending on your environment
              storageClassName: local-storage
              volumeMode: Filesystem 
              accessModes:
                - ReadWriteOnce
    # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement
    # onlyApplyOSDPlacement: false
  healthCheck:
    daemonHealth:
      mon:
        interval: 30s
        timeout: 120s
  resources:
  #  prepareosd:
  #    limits:
  #      cpu: "200m"
  #      memory: "200Mi"
  #   requests:
  #      cpu: "200m"
  #      memory: "200Mi"
  disruptionManagement:
    managePodBudgets: true
    # osdMaintenanceTimeout: 30
    # pgHealthCheckTimeout: 0
    # manageMachineDisruptionBudgets: false
    # machineDisruptionBudgetNamespace: openshift-machine-api
